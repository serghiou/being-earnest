# Being earnest about science: the skeleton in the closet 

The scientific establishment is arguably in a state of turmoil. In fact ‘apocalypse’, which is Greek for ‘revelation’, is a particularly apt term for it. This was probably most explicitly articulated by John Ioannidis in an article published a bit more than a decade ago, titled ‘Why most research findings are false’ (1). Since then, other articles have estimated that roughly 85% of research investment – equating to $200 billion of the investment in 2010 – is wasted (2, 3) and most recently the Cancer Reproducibility Project only managed to broadly support the conclusions of two out of five highly-regarded studies in cancer biology (4). What is most troubling is that problems with how science works are not inconsequential.
One of the most obvious such examples is the now retracted 1998 article in The Lancet by Wakefield et al., which purported that the measles, mumps and rubella (MMR) vaccine is associated with ‘pervasive developmental disorder in children’ (5). These claims were made on the basis of a case series of twelve children, which represent far from the ideal study design, sampling method and sample size to make such a conclusion. The repercussions of that paper have been far-reaching and have impacted public health and spending since then. Another particularly interesting paper by Turner et al., published in the New England Journal of Medicine, revealed how the published literature on antidepressant trials conveys a very skewed image of the truth (6). Even though out of 74 clinical trials registered with the FDA only 38 identified a ‘positive’ result, most trials with a ‘negative’ result were either not published, or altered in such a way as to convey a ‘positive’ message, which meant that, in fact, out of 52 published trials, only 3 conveyed a ‘negative’ result. 
The bullet for these shortcomings is most commonly taken by the researchers. Despite calls for open data and transparency, researchers keep withholding data from open access or, if they do actually release these data, they tend to be in too messy a state to make any sense. Despite calls for appropriate statistical analyses and use of p-values, most prominent of which was the recent statement on p-values by the American Statistical Association (7), researchers keep hunting for p-values below 0.05 and selecting appropriate analyses to achieve that. Despite calls for publishing null effects and approaching published studies systematically rather than cherry-picking the ones most suited to one’s arguments, the grant majority of literature is still one of ‘positive’ effects.
Nevertheless, “The truth is rarely pure and never simple” (Oscar Wilde, The importance of being Earnest). The problem is that researchers, in the grand scheme of science, are a cogwheel within a cogwheel, and humans more than scientists. As humans, they need to balance practicing science with absolute and uncompromising rigor on one hand, with the reality of survival on the other. Our current reward systems (such as getting published or receiving an award) more often than not, promote statistically significant results capable of making headlines, rather than a robust study design, statistical methods and replicability (2). Similarly, funding and tenure, ultimately favor those in well-known universities, within well-known and populous labs, thus capable of making their way into prestigious publications, churning out volumes of research and accumulating the number of citations they necessitate (8). This is the game, and you must play by the rules if you ever hope to win.
So can we blame those who choose to abide by the rules of ‘the game’? Can we blame those who choose to analyze data they accumulated often over the length of a whole year with analysis B rather than analysis A, because analysis B yields a p-value less than 0.05 and immediately improves the chances of reaching a prestigious journal? Can we blame them for not promoting a change? The game has implicitly promised that creating this kind of headline research that can get into prestigious journals and attract citations will secure funding and fame. What would happen if the rules of the game all of a sudden changed? Would these researchers have to work themselves up the ladder again?
I think that we are all to blame for the state of biomedical research quality. Researchers, doctors, funding bodies, journals, readers, politicians, are all to blame. We need to accept that most research is unreliable and often wrong, but we need to realize it could have been nothing but wrong. We are part of a system that selectively penalizes those that choose to abide by integrity and this will remain the case, unless we target the problem at its root. We need scientists working on designing experiments that yield results that approximate reality as closely as possible, without having to balance this with objectives that should not affect science, such as obtaining ‘positive’ results or outcompeting their colleagues. 
However, as a young scientist myself, this is not the kind of research I am being taught by the scientific establishment. The new generation is being taught that publishing your research in anything less than the top 5 journals in terms of impact factor is poor performance. The new generation is being taught that they are not good enough and should not publish until they find a ‘significant’ result. Concurrently, the new generation is not being taught how to design an experiment appropriately by taking into account considerations such as randomization, blinding and validation. It is also not being taught about how to appropriately report their experiments in such a way as to facilitate reanalysis and synthesis of their work within systematic reviews and meta-analyses.
Let us embrace the evolution of the scientific method, teach the younger generation what we now know are important practices in scientific research and reporting and let them practice the kind of science they should. A scientist’s work should be valued for its integrity, its rigor and its maturity, rather than its volume and ability to harness a p-value. Let us be earnest about science.

 
References
1.	Ioannidis JP. Why most published research findings are false. PLoS Med. 2005 Aug;2(8):e124. 
2.	Macleod MR, Michie S, Roberts I, Dirnagl U, Chalmers I, Ioannidis JP, Al-Shahi Salman R, Chan AW, Glasziou P. Biomedical research: increasing value, reducing waste. Lancet. 2014 Jan 11;383(9912):101-4. doi: 10.1016/S0140-6736(13)62329-6. 
3.	Røttingen JA, Regmi S, Eide M, et al. Mapping of available health research and development data: what’s there, what’s missing, and what role is there for a global observatory? Lancet 2013; 382: 1286–307.
4.	eLife. Media coverage: First results of cancer reproducibility project released. eLife. 2017 Jan 19. https://elifesciences.org/elife-news/media-coverage-first-results-cancer-reproducibility-project-released. Accessed 20 April 2017.
5.	Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children. Wakefield AJ, Murch SH, Anthony A, Linnell J, Casson DM, Malik M, Berelowitz M, Dhillon AP, Thomson MA, Harvey P, Valentine A, Davies SE, Walker-Smith JA. Lancet. 1998 Feb 28; 351(9103):637-41.
6.	Turner EH, Matthews AM, Linardatos E, Tell RA, Rosenthal R. Selective publication of antidepressant trials and its influence on apparent efficacy. N Engl J Med. 2008 Jan 17;358(3):252-60. doi: 10.1056/NEJMsa065779.
7.	Wasserstein RL, Lazar NA. The ASA's Statement on p-Values: Context, Process, and Purpose. Am Stat. 2016 Jun 09; 70(2): 129-33
8.	Greenberg SA. How citation distortions create unfounded authority: analysis of a citation network. BMJ. 2009;339:b2680.
