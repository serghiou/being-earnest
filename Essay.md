# Being earnest about science: the skeleton in the closet 

<div align="justify">
 
The scientific establishment is arguably in a state of turmoil. Theoretical estimates of most published findings in biomedicicine being false, such as that published by John Ioannidis a bit more than a decade ago (1), are progressively corroborated by empirical evidence. An attempt by Amgen researchers to repreoduce landmark studies in cancer biology could only reproduce 6/55 studies and a similar attempt by BayerHealth could only adequately validate 14-17/68 of high-profile studies in biomedicine. Indeed, a series of articles published in the famous medical journal The Lancet estimated that roughly 85% of research investment (about $200 billion in 2010), is wasted (2, 3) due to lack of appropriate scientific practice. Unfortunately, these problems are not inconsequential.

One of the most obvious such examples is the now retracted 1998 article in The Lancet by Wakefield et al., which purported that the measles, mumps and rubella (MMR) vaccine is associated with ‘pervasive developmental disorder in children’ (5). These claims were made on the basis of a case series of twelve children, a study design and sample size that simply cannot make such bold claims. Despite multiple refutations of that early finding by better designed and more powerful studies, the repercussions of that paper have been far-reaching and have impacted public health and spending ever since. Another paper by Turner et al., published in the New England Journal of Medicine, it too a prestigious medical journal, revealed how the published literature on anti-depressant trials conveys a very skewed image of the truth (6). Even though out of 74 clinical trials registered with the Food and Drug Administration (FDA) only 38 identified a ‘positive’ result, most trials with a ‘negative’ result were either not published, or altered in such a way as to convey a ‘positive’ message; as such, out of 52 published trials, only 3 conveyed a ‘negative’ result. So, who is to blame for this status quo in biomedical research?

The bullet for these shortcomings is most commonly taken by the researchers. Despite calls for open data and transparency, researchers keep withholding data from open access or, if they do actually release these data, they tend to be in too messy a state to make any sense. Despite calls for appropriate statistical analyses and use of p-values, most prominent of which was the recent statement on p-values by the American Statistical Association (7), researchers keep hunting for p-values below 0.05 and selecting appropriate analyses to achieve that. Despite calls for publishing null effects and approaching published studies systematically rather than cherry-picking the ones most suited to one’s arguments, the grant majority of literature is still one of ‘positive’ effects.

Nevertheless, “The truth is rarely pure and never simple” (Oscar Wilde, The importance of being Earnest). The problem is that researchers, in the grand scheme of science, are a cogwheel within a cogwheel, and humans more than scientists. As humans, they need to balance practicing science with absolute and uncompromising rigor on one hand, with the reality of survival on the other. Our current reward systems (such as getting published or receiving an award) more often than not, promote statistically significant results capable of making headlines, rather than a robust study design, statistical methods and replicability (2). Similarly, funding and tenure, ultimately favor those in well-known universities, within well-known and populous labs, capable of making their way into prestigious publications, churning out volumes of research and accumulating the number of citations they necessitate (8). This is the game.

Can we blame those who choose to abide by the rules of ‘the game’? Can we blame those who choose to analyze data they accumulated often over the length of a whole year with analysis B rather than analysis A, because analysis B yields a p-value less than 0.05 and immediately improves the chances of reaching a prestigious journal? Can we blame them for not promoting a change? The game has implicitly promised that creating this kind of headline research that can get into prestigious journals and attract citations will secure funding and prestige. What would happen if the rules of the game all of a sudden changed? Would these researchers have to work themselves up the ladder again?

I think that we are all to blame for the state of biomedical research quality. Researchers, doctors, funding bodies, journals, readers, politicians, are all to blame. We need to accept that most research is unreliable and often wrong, but we need to realize it could have been nothing but wrong. We are part of a system that selectively penalizes those that choose to abide by integrity, rigor and consientious science and this will remain the case, unless we target the problem at its root. We need scientists working on designing experiments that yield results that approximate reality as closely as possible, without having to balance this with objectives that should not affect science, such as obtaining ‘positive’ results or outcompeting colleagues. 

However, as a young scientist myself, this is not the kind of research I am being taught by the scientific establishment. The new generation is being taught that publishing your research in anything less than the top 5 journals in terms of impact factor is poor performance. The new generation is being taught that they are not good enough and should not publish until they find a ‘significant’ result. Concurrently, the new generation is not being taught how to design an experiment appropriately by taking into account considerations such as randomization, blinding and validation. It is also not being taught about how to appropriately report their experiments in such a way as to facilitate reanalysis and synthesis of their work within systematic reviews and meta-analyses.

Let us embrace the evolution of the scientific method, teach the younger generation what we now know are important practices in scientific research and reporting and let them practice the kind of science they should. A scientist’s work should be valued for its integrity, its rigor and its maturity, rather than its volume and ability to harness a p-value. Let us be earnest about science.

 
## References
1.	Ioannidis JP. Why most published research findings are false. PLoS Med. 2005 Aug;2(8):e124. 
2.	Macleod MR, Michie S, Roberts I, Dirnagl U, Chalmers I, Ioannidis JP, Al-Shahi Salman R, Chan AW, Glasziou P. Biomedical research: increasing value, reducing waste. Lancet. 2014 Jan 11;383(9912):101-4. doi: 10.1016/S0140-6736(13)62329-6. 
3.	Røttingen JA, Regmi S, Eide M, et al. Mapping of available health research and development data: what’s there, what’s missing, and what role is there for a global observatory? Lancet 2013; 382: 1286–307.
4.	eLife. Media coverage: First results of cancer reproducibility project released. eLife. 2017 Jan 19. https://elifesciences.org/elife-news/media-coverage-first-results-cancer-reproducibility-project-released. Accessed 20 April 2017.
5.	Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive developmental disorder in children. Wakefield AJ, Murch SH, Anthony A, Linnell J, Casson DM, Malik M, Berelowitz M, Dhillon AP, Thomson MA, Harvey P, Valentine A, Davies SE, Walker-Smith JA. Lancet. 1998 Feb 28; 351(9103):637-41.
6.	Turner EH, Matthews AM, Linardatos E, Tell RA, Rosenthal R. Selective publication of antidepressant trials and its influence on apparent efficacy. N Engl J Med. 2008 Jan 17;358(3):252-60. doi: 10.1056/NEJMsa065779.
7.	Wasserstein RL, Lazar NA. The ASA's Statement on p-Values: Context, Process, and Purpose. Am Stat. 2016 Jun 09; 70(2): 129-33
8.	Greenberg SA. How citation distortions create unfounded authority: analysis of a citation network. BMJ. 2009;339:b2680.

</div>
